{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774bc389-fc42-43cc-a03c-99c9e59d477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matildagaddi/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchhd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from keras.preprocessing.text import Tokenizer # kills kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85150384-9195-45f9-abd2-b8b88ffbb191",
   "metadata": {},
   "source": [
    "## Load and Merge QA Datatsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1899dff3-1598-44b1-8b8a-6090f79db568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 11679\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dsci = load_dataset(\"allenai/sciq\")\n",
    "print(dsci)\n",
    "dsci_qs = []\n",
    "for i in dsci['train']:\n",
    "    dsci_qs.append({'question': i['question'], 'domain': 'Science'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ab4ad8-05f5-457b-871c-4bb6b1cca077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 182822\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 6150\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 4183\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dmed = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "print(dmed)\n",
    "dmed_qs = []\n",
    "for i in dmed['train']:\n",
    "    dmed_qs.append({'question': i['question'], 'domain': 'Medical'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa44d44-d339-4245-9454-3daa42d9859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['system', 'instruction', 'input', 'output', 'info'],\n",
      "        num_rows: 9189\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['system', 'instruction', 'input', 'output', 'info'],\n",
      "        num_rows: 1022\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dcyber = load_dataset(\"khangmacon/cybermetric-10000\")\n",
    "print(dcyber)\n",
    "dcyber_qs = []\n",
    "for i in dcyber['train']:\n",
    "    dcyber_qs.append({'question': i['input'].split('?')[0], 'domain': 'Cyber'}) #maybe add 'class': '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791ffed8-040c-4d88-80f2-3f49fd6fc588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['financebench_id', 'company', 'doc_name', 'question_type', 'question_reasoning', 'domain_question_num', 'question', 'answer', 'justification', 'dataset_subset_label', 'evidence', 'gics_sector', 'doc_type', 'doc_period', 'doc_link'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfin = load_dataset(\"PatronusAI/financebench\")\n",
    "dfin # only 150 rows, huge class imbalance and not enough to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6404112d-119c-494e-b2f3-fe3d6427d48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203690"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_all = dsci_qs+dmed_qs+dcyber_qs \n",
    "len(d_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded7c91a-6d8b-4a88-a74d-89fa2bb72312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of organism is commonly used in prep...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What phenomenon makes global winds blow northe...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changes from a less-ordered state to a more-or...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the least dangerous radioactive decay?</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kilauea in hawaii is the world’s most continuo...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203685</th>\n",
       "      <td>What is the main goal of the committee set up ...</td>\n",
       "      <td>Cyber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203686</th>\n",
       "      <td>In the context of cybersecurity, what is the m...</td>\n",
       "      <td>Cyber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203687</th>\n",
       "      <td>What is the purpose of the change management p...</td>\n",
       "      <td>Cyber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203688</th>\n",
       "      <td>Which of the following, when removed, can incr...</td>\n",
       "      <td>Cyber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203689</th>\n",
       "      <td>Which principle describes the hacker's manipul...</td>\n",
       "      <td>Cyber</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203690 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question   domain\n",
       "0       What type of organism is commonly used in prep...  Science\n",
       "1       What phenomenon makes global winds blow northe...  Science\n",
       "2       Changes from a less-ordered state to a more-or...  Science\n",
       "3          What is the least dangerous radioactive decay?  Science\n",
       "4       Kilauea in hawaii is the world’s most continuo...  Science\n",
       "...                                                   ...      ...\n",
       "203685  What is the main goal of the committee set up ...    Cyber\n",
       "203686  In the context of cybersecurity, what is the m...    Cyber\n",
       "203687  What is the purpose of the change management p...    Cyber\n",
       "203688  Which of the following, when removed, can incr...    Cyber\n",
       "203689  Which principle describes the hacker's manipul...    Cyber\n",
       "\n",
       "[203690 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_dom = pd.DataFrame(d_all)\n",
    "q_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab0051e-4408-4335-8c13-1610ef504764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to encode with word2vec and word2hypervec, calculate time difference\n",
    "# https://github.com/goktug16/Word2HyperVec-From-Word-Embeddings-to-Hypervectors-for-Hyperdimensional-Computing\n",
    "# classify with NN vs HDC, calculate time difference (also see SOTA works on this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447ed41-de99-476b-b4cf-537f65f6d87c",
   "metadata": {},
   "source": [
    "## Classify with Word2Vec and Logistic Regression\n",
    "### Need to check validity still, not sure if its completely right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e5fa32-0457-41f1-9519-f11a529b67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = [question.split() for question in q_dom['question']]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_questions, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def question_to_vector(question, model):\n",
    "    vectors = [model.wv[word] for word in question.split() if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "question_vectors = np.array([question_to_vector(q, word2vec_model) for q in q_dom['question']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dbee9b9-01ef-4fb5-a6d8-b7179e1c2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(question_vectors, q_dom['domain'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134fc94b-0d96-4fb4-b620-0fa8dfdff623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matildagaddi/miniforge3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cyber       0.86      0.75      0.80      1868\n",
      "     Medical       0.98      0.99      0.98     36530\n",
      "     Science       0.81      0.77      0.79      2340\n",
      "\n",
      "    accuracy                           0.96     40738\n",
      "   macro avg       0.88      0.84      0.86     40738\n",
      "weighted avg       0.96      0.96      0.96     40738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "classifier = LogisticRegression(max_iter=200, random_state=1) #class_weight='balanced' gives worse results\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232babaa-197e-4c30-a6d2-54352f5a9acd",
   "metadata": {},
   "source": [
    "## Classify with HDC\n",
    "### Messy/wrong implementation so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4143d5-6f2f-4aed-9da7-f96ccd1cc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_questions = q_dom['question']\n",
    "domain_labels = q_dom['domain']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(q_dom['question'], q_dom['domain'], test_size=0.2, random_state=1)\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    Compute positional encodings for a given position and embedding size.\n",
    "    \"\"\"\n",
    "    def get_angle(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    angle_rads = get_angle(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis, :],\n",
    "                           d_model)\n",
    "\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    return pos_encoding\n",
    "\n",
    "# def encode_questions_with_position(questions, model):\n",
    "#     \"\"\"\n",
    "#     Encode questions with Word2Vec embeddings and add positional encoding.\n",
    "#     \"\"\"\n",
    "#     encoded_questions = []\n",
    "#     for question in questions:\n",
    "#         words = question.split()  # Simple tokenization by splitting on spaces\n",
    "#         word_embeddings = np.array([\n",
    "#             model.wv[word] if word in model.wv else np.zeros(model.vector_size) \n",
    "#             for word in words\n",
    "#         ])\n",
    "#         pos_encodings = positional_encoding(len(words), model.vector_size)\n",
    "#         encoded_question = word_embeddings + pos_encodings\n",
    "#         encoded_questions.append(encoded_question)\n",
    "#     return encoded_questions\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Normalize embeddings using Min-Max normalization.\n",
    "    \"\"\"\n",
    "    normalized_embeddings = []\n",
    "    for emb in embeddings:\n",
    "        min_val = np.min(emb, axis=0, keepdims=True)\n",
    "        max_val = np.max(emb, axis=0, keepdims=True)\n",
    "        range_val = np.where(max_val - min_val == 0, 1, max_val - min_val)  # Avoid division by zero\n",
    "        normalized_emb = (emb - min_val) / range_val\n",
    "        normalized_embeddings.append(normalized_emb)\n",
    "    return normalized_embeddings\n",
    "\n",
    "\n",
    "model = word2vec_model\n",
    "\n",
    "# #load training and testing data\n",
    "# with open('questions_train_data.pkl', 'rb') as f:\n",
    "#     X_train, y_train = pickle.load(f)\n",
    "\n",
    "# with open('questions_test_data.pkl', 'rb') as f:\n",
    "#     X_test, y_test = pickle.load(f)\n",
    "\n",
    "# Encode questions with positional information for training and testing sets\n",
    "\n",
    "def vec_to_hv():\n",
    "    ...\n",
    "\n",
    "q_hvs = vec_to_hv(question_vectors)\n",
    "X_train, X_test, y_train, y_test = train_test_split(q_hvs, q_dom['domain'], test_size=0.2, random_state=1)\n",
    "\n",
    "X_train_encoded = ... # was 'encode_questions_with_position(X_train, model)' but got errors\n",
    "X_test_encoded = ...\n",
    "\n",
    "#normalize embeddings\n",
    "X_train_norm = normalize_embeddings(X_train_encoded)\n",
    "X_test_norm = normalize_embeddings(X_test_encoded)\n",
    "\n",
    "# (Optional) Convert normalized embeddings into binary hypervectors\n",
    "def binarize_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Convert normalized embeddings to binary hypervectors, retaining the list format.\n",
    "    \"\"\"\n",
    "    binary_embeddings = [np.where(emb > 0.5, 1, 0) for emb in embeddings]\n",
    "    return binary_embeddings\n",
    "\n",
    "# X_train_binary = binarize_embeddings(X_train_norm)\n",
    "# X_test_binary = binarize_embeddings(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49662a5e-41fd-4d93-8fda-6d53c3ef7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_hypervectors(X_train_norm, y_train, num_classes):\n",
    "    \"\"\"\n",
    "    Generate class hypervectors by aggregating training data hypervectors for each class.\n",
    "    \"\"\"\n",
    "    class_hypervectors = [np.zeros(X_train_norm[0].shape, dtype=float) for _ in range(num_classes)]\n",
    "    \n",
    "    for hv, label in zip(X_train_norm, y_train):\n",
    "        class_hypervectors[label] += hv\n",
    "\n",
    "    # print(class_hypervectors)\n",
    "    # # Normalize to binary by thresholding at 50%\n",
    "    # class_hypervectors = [np.where(hv > len(y_train) / (2 * num_classes), 1, 0) for hv in class_hypervectors]\n",
    "    class_hypervectors = [vector / np.linalg.norm(vector) for vector in class_hypervectors]\n",
    "\n",
    "    print(np.array(class_hypervectors))\n",
    "    return np.array(class_hypervectors)\n",
    "\n",
    "def classify_hypervectors(X_test_binary, class_hypervectors):\n",
    "    \"\"\"\n",
    "    Classify test hypervectors by comparing with class hypervectors.\n",
    "    \"\"\"\n",
    "    print(X_test_binary[0])\n",
    "    predictions = []\n",
    "    for test_hv in X_test_binary:\n",
    "        similarities = [1 - cosine_similarity(test_hv.reshape(1, -1), class_hv.reshape(1, -1)) for class_hv in class_hypervectors]\n",
    "        predicted_class = np.argmax(similarities)\n",
    "        predictions.append(predicted_class)\n",
    "    return predictions\n",
    "\n",
    "# Map class labels to integers\n",
    "unique_classes = sorted(set(y_train))  # Get sorted unique classes\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}  # Map class -> index\n",
    "index_to_class = {idx: cls for cls, idx in class_to_index.items()}  # Optional: reverse mapping\n",
    "\n",
    "# Convert y_train and y_test to integer labels\n",
    "y_train_int = [class_to_index[label] for label in y_train]\n",
    "y_test_int = [class_to_index[label] for label in y_test]\n",
    "\n",
    "# Number of classes (update this based on your dataset)\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "# Generate class hypervectors\n",
    "class_hypervectors = generate_class_hypervectors(X_train_norm, y_train_int, num_classes)\n",
    "\n",
    "# Classify test data\n",
    "y_pred_int = classify_hypervectors(X_test_norm, class_hypervectors)\n",
    "\n",
    "# Convert predictions back to original class labels\n",
    "y_pred = [index_to_class[pred] for pred in y_pred_int]\n",
    "\n",
    "# Evaluate performance (e.g., accuracy)\n",
    "accuracy = np.mean(np.array(y_pred) == np.array(y_test))\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb603c2-1e74-4ac8-bf0b-c9558e3e35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a heatmap for better visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfd57c-730a-4127-a795-57e230cb5f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
